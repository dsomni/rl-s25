{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98385ce1",
   "metadata": {},
   "source": [
    "# Actor Critic Algorithm test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe4f5233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "25dde08f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import warnings\n",
    "from itertools import accumulate\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "ROOT_FOLDER = os.path.join(\".\", \"..\")\n",
    "if ROOT_FOLDER not in sys.path:\n",
    "    sys.path.insert(0, ROOT_FOLDER)\n",
    "\n",
    "\n",
    "from dataset import RegexDataset\n",
    "\n",
    "# from environment import Environment, EnvSettings\n",
    "from environment_metrics import Environment, EnvSettings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.disable(logging.WARNING)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975148d8",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "043918c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int = 420):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2c2399",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "206db417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('a2d', [0, 1, 0], 1)\n",
      "('2bb', [1, 0, 0], 1)\n",
      "('a2d', [0, 1, 0], 1)\n",
      "('2bb', [1, 0, 0], 1)\n",
      "('a2d', [0, 1, 0], 1)\n",
      "('2bb', [1, 0, 0], 1)\n",
      "('a2d', [0, 1, 0], 1)\n",
      "('2bb', [1, 0, 0], 1)\n",
      "('a2d', [0, 1, 0], 1)\n",
      "('2bb', [1, 0, 0], 1)\n"
     ]
    }
   ],
   "source": [
    "dataset = RegexDataset([\"a2d\", \"2bb\"], r\"\\d+\")\n",
    "data_iter = dataset.create_iterator()\n",
    "\n",
    "for i in range(10):\n",
    "    print(next(data_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8a8f63",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e9c0192",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = Environment(dataset, settings=EnvSettings(max_steps=2))\n",
    "\n",
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "39cf7f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action=49\n",
      "(array([0.47115385, 1.        ]), 0, False)\n",
      "action=72\n",
      "(array([1., 1.]), -134.63333333333335, True)\n",
      "action=6\n",
      "(array([0.05769231, 1.        ]), 0, False)\n",
      "action=63\n",
      "(array([1., 1.]), -134.63333333333335, True)\n",
      "action=87\n",
      "(array([0.83653846, 1.        ]), 0, False)\n",
      "action=53\n",
      "(array([1., 1.]), -134.63333333333335, True)\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "for _ in range(6):\n",
    "    action = np.random.randint(env.action_space)\n",
    "    print(f\"{action=}\")\n",
    "    print(env.step(action))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5a6a34",
   "metadata": {},
   "source": [
    "## Advantage Actor-Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b7d07017",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_qvals(\n",
    "    rewards: list[float] | np.ndarray, gamma: float = 1.0, reward_steps: int = 0\n",
    ") -> np.ndarray:\n",
    "    rw_steps = reward_steps if reward_steps != 0 else len(rewards)\n",
    "\n",
    "    return np.array(\n",
    "        [\n",
    "            list(\n",
    "                accumulate(\n",
    "                    reversed(rewards[i : i + rw_steps]), lambda x, y: gamma * x + y\n",
    "                )\n",
    "            )[-1]\n",
    "            for i in range(len(rewards))\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3d974217",
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2CNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int = env.state_space,\n",
    "        output_dim: int = env.action_space,\n",
    "        hidden_dim: int = 128,\n",
    "    ) -> None:\n",
    "        super(A2CNet, self).__init__()\n",
    "\n",
    "        self.body = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.policy = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim),\n",
    "        )\n",
    "\n",
    "        self.value = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "        )\n",
    "\n",
    "        nn.init.xavier_uniform_(self.policy[-1].weight, gain=0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        body_out = self.body(x)\n",
    "        return self.policy(body_out), self.value(body_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c4feb9",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8e507191",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, temperature_coefficient: float = 10.0):\n",
    "        self.temperature_coefficient = temperature_coefficient\n",
    "\n",
    "    def choose_action(self, action_logits: torch.Tensor, epoch: int):\n",
    "        temperature = (\n",
    "            1 / epoch * torch.max(torch.abs(action_logits)) * self.temperature_coefficient\n",
    "            if self.temperature_coefficient > 0\n",
    "            else 1\n",
    "        )\n",
    "\n",
    "        return np.random.choice(\n",
    "            range(len(action_logits)),\n",
    "            size=1,\n",
    "            p=F.softmax(action_logits / temperature, dim=0).numpy(),\n",
    "        )[0]\n",
    "\n",
    "    def choose_optimal_action(self, action_logits: torch.Tensor) -> int:\n",
    "        return int(np.argmax(F.softmax(action_logits, dim=0).cpu()).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f332a3",
   "metadata": {},
   "source": [
    "## Trajectory Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f948ed02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrajectoryBuffer:\n",
    "    \"\"\"\n",
    "    Buffer class to store the experience from a unique policy\n",
    "    \"\"\"\n",
    "\n",
    "    def _batch(self, iterable):\n",
    "        ln = len(iterable)\n",
    "        for ndx in range(0, ln, self.batch_size):\n",
    "            yield iterable[ndx : min(ndx + self.batch_size, ln)]\n",
    "\n",
    "    def __init__(self, batch_size: int = 64):\n",
    "        self.batch_size = batch_size\n",
    "        self.clean()\n",
    "\n",
    "    def clean(self):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.discounted_rewards = []\n",
    "\n",
    "    def store(\n",
    "        self,\n",
    "        states_trajectory: np.ndarray,\n",
    "        trajectory: np.ndarray,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Add trajectory values to the buffers and compute the advantage and reward to go\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        states_trajectory:  list that contains states\n",
    "        trajectory: list where each element is a list that contains: reward, action\n",
    "        \"\"\"\n",
    "        assert len(states_trajectory) == len(trajectory)\n",
    "\n",
    "        if len(states_trajectory) > 0:\n",
    "            self.states.extend(states_trajectory)\n",
    "            self.actions.extend(trajectory[:, 1])\n",
    "\n",
    "            self.discounted_rewards.extend(calculate_qvals(trajectory[:, 0]))\n",
    "\n",
    "    def get_batches(self, mean_baseline: bool):\n",
    "        mean_rewards = np.mean(self.discounted_rewards) if mean_baseline else 0\n",
    "\n",
    "        for states_batch, actions_batch, discounted_rewards_batch in zip(\n",
    "            self._batch(self.states),\n",
    "            self._batch(self.actions),\n",
    "            self._batch(self.discounted_rewards),\n",
    "        ):\n",
    "            mean_batch_reward = np.mean(discounted_rewards_batch) if mean_baseline else 0\n",
    "            yield (\n",
    "                torch.tensor(states_batch, dtype=torch.float32, device=DEVICE),\n",
    "                torch.tensor(actions_batch, dtype=torch.long, device=DEVICE),\n",
    "                torch.tensor(\n",
    "                    np.array(discounted_rewards_batch) - mean_rewards,\n",
    "                    # np.array(discounted_rewards_batch) - mean_batch_reward,\n",
    "                    dtype=torch.float,\n",
    "                    device=DEVICE,\n",
    "                ),\n",
    "            )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.states)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b04482",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7dbf846a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_buffer(\n",
    "    a2c_net: nn.Module, agent: Agent, buffer: TrajectoryBuffer, episodes: int, epoch: int\n",
    "):\n",
    "    buffer.clean()\n",
    "    state = env.reset()\n",
    "    done_episodes = 0\n",
    "    ep_states_buf, ep_rew_act_buf = [], []\n",
    "\n",
    "    train_rewards = []\n",
    "\n",
    "    epoch_loop = tqdm(total=episodes, desc=f\"Epoch #{epoch}\", position=0, disable=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        while done_episodes < episodes:\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "            action_logits, _ = a2c_net(state_tensor)\n",
    "\n",
    "            action = agent.choose_action(action_logits, epoch=epoch)\n",
    "            next_state, reward, done = env.step(action)\n",
    "\n",
    "            ep_states_buf.append(state)\n",
    "            ep_rew_act_buf.append([reward, int(action)])\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                buffer.store(\n",
    "                    np.array(ep_states_buf),\n",
    "                    np.array(ep_rew_act_buf),\n",
    "                )\n",
    "\n",
    "                ep_states_buf, ep_rew_act_buf = [], []\n",
    "\n",
    "                train_rewards.append(reward)\n",
    "\n",
    "                done_episodes += 1\n",
    "                epoch_loop.update(1)\n",
    "\n",
    "    return train_rewards\n",
    "\n",
    "\n",
    "def train(\n",
    "    a2c_net: nn.Module,\n",
    "    a2c_optimizer: optim.Optimizer,\n",
    "    buffer: TrajectoryBuffer,\n",
    "    mean_baseline: bool = True,\n",
    "    entropy_beta: float = 1e-3,\n",
    "    clip_grad: float = 10,\n",
    "):\n",
    "    a2c_net.train()\n",
    "    losses = []\n",
    "    entropies = []\n",
    "    for batch in buffer.get_batches(mean_baseline):\n",
    "        a2c_optimizer.zero_grad()\n",
    "        (\n",
    "            state_batch,\n",
    "            action_batch,\n",
    "            reward_batch,\n",
    "        ) = batch\n",
    "\n",
    "        logits_v, value_v = a2c_net(state_batch)\n",
    "\n",
    "        # Value loss\n",
    "        loss_value_v = F.mse_loss(value_v.squeeze(-1), reward_batch)\n",
    "\n",
    "        # Policy loss\n",
    "        log_prob_v = F.log_softmax(logits_v, dim=1)\n",
    "        adv_v = reward_batch - value_v.detach()\n",
    "        log_prob_actions_v = adv_v * log_prob_v[range(len(state_batch)), action_batch]\n",
    "        loss_policy_v = -log_prob_actions_v.mean()\n",
    "\n",
    "        # Entropy loss\n",
    "        prob_v = F.softmax(logits_v, dim=1)\n",
    "        entropy_v = -(prob_v * log_prob_v).sum(dim=1).mean()\n",
    "        entropy_loss_v = entropy_beta * entropy_v\n",
    "        loss_policy_v = loss_policy_v - entropy_loss_v\n",
    "\n",
    "        # Policy backward\n",
    "        loss_policy_v.backward(retain_graph=True)\n",
    "\n",
    "        # Value backward\n",
    "        loss_v = loss_value_v - entropy_loss_v\n",
    "        loss_v.backward()\n",
    "\n",
    "        if clip_grad > 0:\n",
    "            nn.utils.clip_grad_norm_(a2c_net.parameters(), clip_grad)\n",
    "\n",
    "        a2c_optimizer.step()\n",
    "\n",
    "        losses.append(loss_v.item() + loss_policy_v.item())\n",
    "        entropies.append(entropy_v.item())\n",
    "\n",
    "    return losses, entropies\n",
    "\n",
    "\n",
    "def evaluate(\n",
    "    a2c_net: nn.Module,\n",
    "    env: Environment,\n",
    "    agent: Agent,\n",
    ") -> tuple[str, float]:\n",
    "    a2c_net.eval()\n",
    "    max_steps = env.settings.max_steps\n",
    "    regex_actions = []\n",
    "    total_reward = 0\n",
    "\n",
    "    state = env.reset()\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(env)):\n",
    "            regex_actions = []\n",
    "            for _ in range(max_steps):\n",
    "                state_tensor = torch.tensor(state, dtype=torch.float32, device=DEVICE)\n",
    "                action_logits, _ = a2c_net(state_tensor)\n",
    "\n",
    "                action = agent.choose_optimal_action(action_logits)\n",
    "                regex_actions.append(env.idx_to_action(action))\n",
    "\n",
    "                next_state, reward, done = env.step(action)\n",
    "\n",
    "                state = next_state\n",
    "                if done:\n",
    "                    total_reward += reward\n",
    "                    break\n",
    "\n",
    "    if regex_actions and regex_actions[-1] == env.finish_action:\n",
    "        regex_actions = regex_actions[:-1]\n",
    "\n",
    "    try:\n",
    "        regex = env.rpn.to_infix(regex_actions)\n",
    "    except BaseException:\n",
    "        regex = f\"Invalid: {regex_actions}\"\n",
    "\n",
    "    return regex, total_reward\n",
    "\n",
    "\n",
    "def train_eval_loop(\n",
    "    a2c_net: nn.Module,\n",
    "    a2c_optimizer: optim.Optimizer,\n",
    "    agent: Agent,\n",
    "    buffer: TrajectoryBuffer,\n",
    "    epochs: int,\n",
    "    episodes: int,\n",
    "    mean_baseline: bool = True,\n",
    "    entropy_beta: float = 0.5,\n",
    "    eval_period: int = 5,\n",
    "    clip_grad: float = 10,\n",
    "):\n",
    "    set_seed()\n",
    "\n",
    "    for i in range(1, epochs + 1):\n",
    "        train_rewards = fill_buffer(a2c_net, agent, buffer, episodes, epoch=i)\n",
    "\n",
    "        losses, entropies = train(\n",
    "            a2c_net, a2c_optimizer, buffer, mean_baseline, entropy_beta, clip_grad\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {i: >3}/{epochs}:\"\n",
    "            f\"\\tReward: {np.mean(train_rewards):.1f}\"\n",
    "            f\"\\tLoss: {np.mean(losses):.3f}\"\n",
    "            f\"\\tEntropy: {np.mean(entropies):.3f}\"\n",
    "        )\n",
    "\n",
    "        if (i % eval_period == 0) or (eval_period == (epochs + 1)):\n",
    "            built_regex, total_reward = evaluate(a2c_net, env, agent)\n",
    "\n",
    "            print(f\"\\nEVALUATION\\nRegex: {built_regex}\\nTotal reward: {total_reward}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "86f88aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_buffer_pre_train(buffer: TrajectoryBuffer, action_str: str):\n",
    "    buffer.clean()\n",
    "    state = env.reset()\n",
    "    ep_states_buf, ep_rew_act_buf = [], []\n",
    "    fin_action = env.action_to_idx(env.finish_action)\n",
    "\n",
    "    for _ in range(len(env)):\n",
    "        action = env.action_to_idx(action_str)\n",
    "\n",
    "        next_state, reward, done = env.step(action)\n",
    "        ep_states_buf.append(state)\n",
    "        ep_rew_act_buf.append([reward, int(action)])\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        next_state, reward, done = env.step(fin_action)\n",
    "        ep_states_buf.append(state)\n",
    "        ep_rew_act_buf.append([reward, int(action)])\n",
    "\n",
    "        buffer.store(\n",
    "            np.array(ep_states_buf),\n",
    "            np.array(ep_rew_act_buf),\n",
    "        )\n",
    "\n",
    "\n",
    "def train_pre_train(\n",
    "    a2c_net: nn.Module,\n",
    "    a2c_optimizer: optim.Optimizer,\n",
    "    buffer: TrajectoryBuffer,\n",
    "    mean_baseline: bool = True,\n",
    "    entropy_beta: float = 1e-3,\n",
    "):\n",
    "    a2c_net.train()\n",
    "    losses = []\n",
    "    entropies = []\n",
    "    for batch in buffer.get_batches(mean_baseline):\n",
    "        a2c_optimizer.zero_grad()\n",
    "        (\n",
    "            state_batch,\n",
    "            action_batch,\n",
    "            reward_batch,\n",
    "        ) = batch\n",
    "\n",
    "        logits_v, value_v = a2c_net(state_batch)\n",
    "\n",
    "        # Value loss\n",
    "        loss_value_v = F.mse_loss(value_v.squeeze(-1), reward_batch)\n",
    "\n",
    "        # Policy loss\n",
    "        log_prob_v = F.log_softmax(logits_v, dim=1)\n",
    "        adv_v = reward_batch - value_v.detach()\n",
    "        log_prob_actions_v = adv_v * log_prob_v[range(len(state_batch)), action_batch]\n",
    "        loss_policy_v = -log_prob_actions_v.mean()\n",
    "\n",
    "        # Entropy loss\n",
    "        prob_v = F.softmax(logits_v, dim=1)\n",
    "        entropy_v = -(prob_v * log_prob_v).sum(dim=1).mean()\n",
    "        entropy_loss_v = entropy_beta * entropy_v\n",
    "        loss_policy_v = loss_policy_v - entropy_loss_v\n",
    "\n",
    "        # Policy backward\n",
    "        loss_policy_v.backward(retain_graph=True)\n",
    "\n",
    "        # Value backward\n",
    "        loss_v = loss_value_v - entropy_loss_v\n",
    "        loss_v.backward()\n",
    "\n",
    "        a2c_optimizer.step()\n",
    "\n",
    "        losses.append(loss_v.item() + loss_policy_v.item())\n",
    "        entropies.append(entropy_v.item())\n",
    "\n",
    "    return losses, entropies\n",
    "\n",
    "\n",
    "def pre_train_eval_loop(\n",
    "    a2c_net: nn.Module,\n",
    "    a2c_optimizer: optim.Optimizer,\n",
    "    mean_baseline: bool = True,\n",
    "    epochs: int = 10,\n",
    "):\n",
    "    set_seed()\n",
    "    for i in tqdm(range(1, epochs + 1)):\n",
    "        buffer = TrajectoryBuffer()\n",
    "        for action in env.actions:\n",
    "            if action == env.finish_action:\n",
    "                continue\n",
    "            fill_buffer_pre_train(buffer, action)\n",
    "\n",
    "            train_pre_train(\n",
    "                a2c_net, a2c_optimizer, buffer, mean_baseline, entropy_beta=1.0\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5caa7677",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()\n",
    "env = Environment(\n",
    "    # RegexDataset([\"a2d\", \"2bb\", \"cc2\", \"d3d\"], r\"\\d+\"), settings=EnvSettings(max_steps=5)\n",
    "    RegexDataset([\"a2d\", \"2bb\", \"cc2\"], r\"\\d+\"),\n",
    "    settings=EnvSettings(max_steps=5),\n",
    ")\n",
    "\n",
    "agent = Agent(temperature_coefficient=0)\n",
    "buffer = TrajectoryBuffer(batch_size=64)\n",
    "\n",
    "a2c_net = A2CNet(input_dim=env.state_space, output_dim=env.action_space).to(DEVICE)\n",
    "a2c_optimizer = optim.Adam(a2c_net.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bc0c348e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre_train_eval_loop(a2c_net, a2c_optimizer, mean_baseline=True, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ce99e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/300:\tReward: -129.9\tLoss: 289.870\tEntropy: 4.643\n",
      "Epoch   2/300:\tReward: -129.6\tLoss: 251.466\tEntropy: 4.642\n",
      "Epoch   3/300:\tReward: -128.0\tLoss: 253.649\tEntropy: 4.630\n",
      "Epoch   4/300:\tReward: -125.6\tLoss: 617.497\tEntropy: 4.572\n",
      "Epoch   5/300:\tReward: -120.7\tLoss: 359.660\tEntropy: 4.266\n",
      "Epoch   6/300:\tReward: -106.4\tLoss: 374.930\tEntropy: 3.241\n",
      "Epoch   7/300:\tReward: -100.1\tLoss: 11.215\tEntropy: 2.221\n",
      "Epoch   8/300:\tReward: -100.1\tLoss: 1.629\tEntropy: 2.199\n",
      "Epoch   9/300:\tReward: -100.1\tLoss: 1.452\tEntropy: 2.223\n",
      "Epoch  10/300:\tReward: -100.1\tLoss: 2.870\tEntropy: 2.262\n",
      "Epoch  11/300:\tReward: -100.1\tLoss: 1.431\tEntropy: 2.203\n",
      "Epoch  12/300:\tReward: -100.2\tLoss: 5.209\tEntropy: 2.175\n",
      "Epoch  13/300:\tReward: -100.1\tLoss: 1.276\tEntropy: 2.110\n",
      "Epoch  14/300:\tReward: -100.1\tLoss: 5.591\tEntropy: 2.099\n",
      "Epoch  15/300:\tReward: -100.0\tLoss: -0.148\tEntropy: 2.133\n",
      "Epoch  16/300:\tReward: -100.1\tLoss: 1.421\tEntropy: 2.177\n",
      "Epoch  17/300:\tReward: -100.1\tLoss: 2.773\tEntropy: 2.187\n",
      "Epoch  18/300:\tReward: -100.0\tLoss: 0.947\tEntropy: 2.225\n",
      "Epoch  19/300:\tReward: -100.0\tLoss: 0.540\tEntropy: 2.244\n",
      "Epoch  20/300:\tReward: -100.0\tLoss: 1.004\tEntropy: 2.250\n",
      "\n",
      "EVALUATION\n",
      "Regex: Invalid: ['concat', 'concat', 'concat', 'concat', '|', 'concat_all']\n",
      "Total reward: -300\n",
      "\n",
      "Epoch  21/300:\tReward: -100.3\tLoss: 12.926\tEntropy: 2.199\n",
      "Epoch  22/300:\tReward: -100.0\tLoss: 1.014\tEntropy: 2.096\n",
      "Epoch  23/300:\tReward: -100.0\tLoss: 1.013\tEntropy: 2.153\n",
      "Epoch  24/300:\tReward: -100.1\tLoss: 1.473\tEntropy: 2.209\n",
      "Epoch  25/300:\tReward: -100.0\tLoss: 1.036\tEntropy: 2.236\n",
      "Epoch  26/300:\tReward: -100.1\tLoss: 1.440\tEntropy: 2.247\n",
      "Epoch  27/300:\tReward: -100.0\tLoss: 1.167\tEntropy: 2.308\n",
      "Epoch  28/300:\tReward: -100.1\tLoss: 4.428\tEntropy: 2.314\n",
      "Epoch  29/300:\tReward: -99.9\tLoss: 42.758\tEntropy: 2.254\n",
      "Epoch  30/300:\tReward: -100.0\tLoss: 0.649\tEntropy: 2.303\n",
      "Epoch  31/300:\tReward: -100.1\tLoss: 1.413\tEntropy: 2.285\n",
      "Epoch  32/300:\tReward: -100.0\tLoss: -0.078\tEntropy: 2.350\n",
      "Epoch  33/300:\tReward: -100.1\tLoss: 2.592\tEntropy: 2.380\n",
      "Epoch  34/300:\tReward: -100.1\tLoss: 10.851\tEntropy: 2.294\n",
      "Epoch  35/300:\tReward: -100.0\tLoss: -0.171\tEntropy: 2.326\n",
      "Epoch  36/300:\tReward: -100.2\tLoss: 4.979\tEntropy: 2.378\n",
      "Epoch  37/300:\tReward: -100.1\tLoss: 2.060\tEntropy: 2.203\n",
      "Epoch  38/300:\tReward: -100.0\tLoss: 0.974\tEntropy: 2.253\n",
      "Epoch  39/300:\tReward: -100.0\tLoss: 1.173\tEntropy: 2.293\n",
      "Epoch  40/300:\tReward: -100.0\tLoss: -0.077\tEntropy: 2.373\n",
      "\n",
      "EVALUATION\n",
      "Regex: Invalid: ['|', '|', '|', '|', '|', 'concat_all']\n",
      "Total reward: -300\n",
      "\n",
      "Epoch  41/300:\tReward: -100.1\tLoss: 1.386\tEntropy: 2.446\n",
      "Epoch  42/300:\tReward: -100.1\tLoss: 2.978\tEntropy: 2.390\n",
      "Epoch  43/300:\tReward: -100.1\tLoss: 1.232\tEntropy: 2.312\n",
      "Epoch  44/300:\tReward: -100.1\tLoss: 3.407\tEntropy: 2.328\n",
      "Epoch  45/300:\tReward: -100.0\tLoss: 1.020\tEntropy: 2.280\n",
      "Epoch  46/300:\tReward: -100.0\tLoss: 1.044\tEntropy: 2.329\n",
      "Epoch  47/300:\tReward: -100.1\tLoss: 0.959\tEntropy: 2.377\n",
      "Epoch  48/300:\tReward: -100.0\tLoss: 1.037\tEntropy: 2.344\n",
      "Epoch  49/300:\tReward: -100.0\tLoss: 0.231\tEntropy: 2.390\n",
      "Epoch  50/300:\tReward: -100.0\tLoss: 0.229\tEntropy: 2.466\n",
      "Epoch  51/300:\tReward: -100.1\tLoss: 56.189\tEntropy: 2.367\n",
      "Epoch  52/300:\tReward: -100.0\tLoss: 0.432\tEntropy: 2.322\n",
      "Epoch  53/300:\tReward: -100.0\tLoss: -0.062\tEntropy: 2.363\n",
      "Epoch  54/300:\tReward: -100.2\tLoss: 7.633\tEntropy: 2.312\n",
      "Epoch  55/300:\tReward: -100.0\tLoss: 0.892\tEntropy: 2.287\n",
      "Epoch  56/300:\tReward: -100.1\tLoss: 2.286\tEntropy: 2.367\n",
      "Epoch  57/300:\tReward: -100.1\tLoss: 3.290\tEntropy: 2.352\n",
      "Epoch  58/300:\tReward: -100.0\tLoss: -0.102\tEntropy: 2.350\n",
      "Epoch  59/300:\tReward: -100.0\tLoss: -0.047\tEntropy: 2.483\n",
      "Epoch  60/300:\tReward: -99.8\tLoss: 86.991\tEntropy: 2.503\n",
      "\n",
      "EVALUATION\n",
      "Regex: Invalid: ['|', '|', '|', '|', '|', 'concat_all']\n",
      "Total reward: -300\n",
      "\n",
      "Epoch  61/300:\tReward: -100.1\tLoss: 2.693\tEntropy: 2.405\n",
      "Epoch  62/300:\tReward: -100.0\tLoss: -0.088\tEntropy: 2.393\n",
      "Epoch  63/300:\tReward: -100.0\tLoss: 1.073\tEntropy: 2.462\n",
      "Epoch  64/300:\tReward: -99.9\tLoss: 43.866\tEntropy: 2.579\n",
      "Epoch  65/300:\tReward: -100.0\tLoss: 45.071\tEntropy: 2.502\n",
      "Epoch  66/300:\tReward: -100.1\tLoss: 2.434\tEntropy: 2.380\n",
      "Epoch  67/300:\tReward: -100.1\tLoss: 2.086\tEntropy: 2.352\n",
      "Epoch  68/300:\tReward: -100.1\tLoss: 1.505\tEntropy: 2.383\n",
      "Epoch  69/300:\tReward: -100.1\tLoss: 3.730\tEntropy: 2.331\n",
      "Epoch  70/300:\tReward: -100.1\tLoss: 1.411\tEntropy: 2.250\n",
      "Epoch  71/300:\tReward: -99.8\tLoss: 39.236\tEntropy: 2.299\n",
      "Epoch  72/300:\tReward: -100.2\tLoss: 6.021\tEntropy: 2.374\n",
      "Epoch  73/300:\tReward: -99.8\tLoss: 43.224\tEntropy: 2.244\n",
      "Epoch  74/300:\tReward: -100.0\tLoss: 0.116\tEntropy: 2.386\n",
      "Epoch  75/300:\tReward: -100.1\tLoss: 2.158\tEntropy: 2.464\n",
      "Epoch  76/300:\tReward: -99.9\tLoss: 41.491\tEntropy: 2.514\n",
      "Epoch  77/300:\tReward: -99.7\tLoss: 81.781\tEntropy: 2.624\n",
      "Epoch  78/300:\tReward: -100.4\tLoss: 73.388\tEntropy: 2.544\n",
      "Epoch  79/300:\tReward: -99.9\tLoss: 41.789\tEntropy: 2.326\n",
      "Epoch  80/300:\tReward: -99.8\tLoss: 41.705\tEntropy: 2.432\n",
      "\n",
      "EVALUATION\n",
      "Regex: Invalid: ['|', '|', '|', '|', '|', 'concat_all']\n",
      "Total reward: -300\n",
      "\n",
      "Epoch  81/300:\tReward: -99.9\tLoss: 42.539\tEntropy: 2.444\n",
      "Epoch  82/300:\tReward: -100.1\tLoss: 10.152\tEntropy: 2.480\n",
      "Epoch  83/300:\tReward: -100.2\tLoss: 66.454\tEntropy: 2.402\n",
      "Epoch  84/300:\tReward: -100.1\tLoss: 2.020\tEntropy: 2.243\n",
      "Epoch  85/300:\tReward: -100.0\tLoss: 1.171\tEntropy: 2.216\n",
      "Epoch  86/300:\tReward: -100.1\tLoss: 1.519\tEntropy: 2.240\n",
      "Epoch  87/300:\tReward: -99.8\tLoss: 39.117\tEntropy: 2.287\n",
      "Epoch  88/300:\tReward: -100.0\tLoss: 0.615\tEntropy: 2.367\n",
      "Epoch  89/300:\tReward: -100.1\tLoss: 10.267\tEntropy: 2.408\n",
      "Epoch  90/300:\tReward: -100.2\tLoss: 12.829\tEntropy: 2.340\n",
      "Epoch  91/300:\tReward: -100.0\tLoss: -0.199\tEntropy: 2.347\n",
      "Epoch  92/300:\tReward: -100.0\tLoss: 53.403\tEntropy: 2.439\n",
      "Epoch  93/300:\tReward: -100.1\tLoss: 1.346\tEntropy: 2.465\n",
      "Epoch  94/300:\tReward: -99.9\tLoss: 43.401\tEntropy: 2.482\n",
      "Epoch  95/300:\tReward: -100.1\tLoss: 58.402\tEntropy: 2.448\n",
      "Epoch  96/300:\tReward: -99.9\tLoss: 47.321\tEntropy: 2.324\n",
      "Epoch  97/300:\tReward: -99.8\tLoss: 92.842\tEntropy: 2.365\n",
      "Epoch  98/300:\tReward: -99.8\tLoss: 92.796\tEntropy: 2.446\n",
      "Epoch  99/300:\tReward: -99.6\tLoss: 126.619\tEntropy: 2.497\n",
      "Epoch 100/300:\tReward: -99.4\tLoss: 162.039\tEntropy: 2.614\n",
      "\n",
      "EVALUATION\n",
      "Regex: [([([([([])])])])]\n",
      "Total reward: -300\n",
      "\n",
      "Epoch 101/300:\tReward: -97.7\tLoss: 722.603\tEntropy: 2.532\n",
      "Epoch 102/300:\tReward: -91.3\tLoss: 1793.535\tEntropy: 1.495\n",
      "Epoch 103/300:\tReward: -98.5\tLoss: 5595.749\tEntropy: 0.472\n",
      "Epoch 104/300:\tReward: -99.2\tLoss: 154.388\tEntropy: 0.215\n",
      "Epoch 105/300:\tReward: -99.6\tLoss: 77.256\tEntropy: 0.278\n",
      "Epoch 106/300:\tReward: -99.4\tLoss: 114.986\tEntropy: 0.318\n",
      "Epoch 107/300:\tReward: -99.3\tLoss: 154.460\tEntropy: 0.389\n",
      "Epoch 108/300:\tReward: -98.5\tLoss: 308.971\tEntropy: 0.598\n",
      "Epoch 109/300:\tReward: -93.1\tLoss: 1333.218\tEntropy: 1.346\n",
      "Epoch 110/300:\tReward: -88.1\tLoss: 6480.735\tEntropy: 1.505\n",
      "Epoch 111/300:\tReward: -94.8\tLoss: 1086.475\tEntropy: 1.500\n",
      "Epoch 112/300:\tReward: -89.4\tLoss: 2110.239\tEntropy: 1.494\n",
      "Epoch 113/300:\tReward: -58.6\tLoss: 7531.477\tEntropy: 1.174\n",
      "Epoch 114/300:\tReward: -56.6\tLoss: 8048.640\tEntropy: 0.862\n",
      "Epoch 115/300:\tReward: -62.5\tLoss: 6255.202\tEntropy: 0.816\n",
      "Epoch 116/300:\tReward: -120.7\tLoss: 2461.426\tEntropy: 0.662\n",
      "Epoch 117/300:\tReward: -71.9\tLoss: 4866.161\tEntropy: 0.925\n",
      "Epoch 118/300:\tReward: -98.6\tLoss: 5696.772\tEntropy: 0.805\n",
      "Epoch 119/300:\tReward: -87.1\tLoss: 2379.799\tEntropy: 0.954\n",
      "Epoch 120/300:\tReward: -63.0\tLoss: 8480.214\tEntropy: 0.912\n",
      "\n",
      "EVALUATION\n",
      "Regex: Invalid: ['?', '?', '?', '?', '?', 'concat_all']\n",
      "Total reward: -300\n",
      "\n",
      "Epoch 121/300:\tReward: -74.2\tLoss: 4743.905\tEntropy: 0.902\n",
      "Epoch 122/300:\tReward: -94.8\tLoss: 5967.485\tEntropy: 0.799\n",
      "Epoch 123/300:\tReward: -84.6\tLoss: 2808.813\tEntropy: 0.826\n",
      "Epoch 124/300:\tReward: -53.2\tLoss: 8993.946\tEntropy: 0.917\n",
      "Epoch 125/300:\tReward: -64.5\tLoss: 5852.252\tEntropy: 0.895\n",
      "Epoch 126/300:\tReward: -87.0\tLoss: 7096.434\tEntropy: 0.774\n",
      "Epoch 127/300:\tReward: -88.6\tLoss: 2090.272\tEntropy: 0.752\n",
      "Epoch 128/300:\tReward: -59.4\tLoss: 6744.429\tEntropy: 0.928\n",
      "Epoch 129/300:\tReward: -93.0\tLoss: 6158.026\tEntropy: 0.819\n",
      "Epoch 130/300:\tReward: -77.7\tLoss: 4287.026\tEntropy: 0.962\n",
      "Epoch 131/300:\tReward: -61.7\tLoss: 9267.170\tEntropy: 0.855\n",
      "Epoch 132/300:\tReward: -81.4\tLoss: 3282.729\tEntropy: 0.900\n",
      "Epoch 133/300:\tReward: -60.3\tLoss: 8871.561\tEntropy: 0.877\n",
      "Epoch 134/300:\tReward: -75.5\tLoss: 4282.470\tEntropy: 0.897\n",
      "Epoch 135/300:\tReward: -61.9\tLoss: 9027.610\tEntropy: 0.870\n",
      "Epoch 136/300:\tReward: -76.4\tLoss: 4280.395\tEntropy: 0.935\n",
      "Epoch 137/300:\tReward: -66.9\tLoss: 8214.510\tEntropy: 0.908\n",
      "Epoch 138/300:\tReward: -69.4\tLoss: 5121.497\tEntropy: 0.941\n",
      "Epoch 139/300:\tReward: -69.1\tLoss: 8512.886\tEntropy: 0.876\n",
      "Epoch 140/300:\tReward: -76.7\tLoss: 4036.315\tEntropy: 0.903\n",
      "\n",
      "EVALUATION\n",
      "Regex: 22222\n",
      "Total reward: -404.8\n",
      "\n",
      "Epoch 141/300:\tReward: -52.4\tLoss: 9075.649\tEntropy: 0.842\n",
      "Epoch 142/300:\tReward: -71.8\tLoss: 4744.411\tEntropy: 0.857\n",
      "Epoch 143/300:\tReward: -87.6\tLoss: 6756.908\tEntropy: 0.788\n",
      "Epoch 144/300:\tReward: -74.9\tLoss: 4325.464\tEntropy: 0.900\n",
      "Epoch 145/300:\tReward: -57.3\tLoss: 9047.115\tEntropy: 0.860\n",
      "Epoch 146/300:\tReward: -71.0\tLoss: 4942.705\tEntropy: 0.869\n",
      "Epoch 147/300:\tReward: -90.7\tLoss: 6845.689\tEntropy: 0.827\n",
      "Epoch 148/300:\tReward: -74.6\tLoss: 4269.605\tEntropy: 0.964\n",
      "Epoch 149/300:\tReward: -59.1\tLoss: 8785.364\tEntropy: 0.912\n",
      "Epoch 150/300:\tReward: -66.8\tLoss: 5375.833\tEntropy: 0.989\n",
      "Epoch 151/300:\tReward: -64.1\tLoss: 8823.315\tEntropy: 0.930\n",
      "Epoch 152/300:\tReward: -65.4\tLoss: 5712.454\tEntropy: 0.999\n",
      "Epoch 153/300:\tReward: -65.5\tLoss: 9167.371\tEntropy: 0.915\n",
      "Epoch 154/300:\tReward: -72.4\tLoss: 4767.311\tEntropy: 1.061\n",
      "Epoch 155/300:\tReward: -54.4\tLoss: 8865.499\tEntropy: 0.940\n",
      "Epoch 156/300:\tReward: -60.4\tLoss: 6252.296\tEntropy: 0.953\n",
      "Epoch 157/300:\tReward: -68.9\tLoss: 8592.218\tEntropy: 0.901\n",
      "Epoch 158/300:\tReward: -67.8\tLoss: 5268.665\tEntropy: 0.963\n",
      "Epoch 159/300:\tReward: -63.1\tLoss: 8886.030\tEntropy: 0.883\n",
      "Epoch 160/300:\tReward: -72.5\tLoss: 4515.661\tEntropy: 0.907\n",
      "\n",
      "EVALUATION\n",
      "Regex: 22222\n",
      "Total reward: -404.8\n",
      "\n",
      "Epoch 161/300:\tReward: -57.1\tLoss: 9256.005\tEntropy: 0.814\n",
      "Epoch 162/300:\tReward: -66.0\tLoss: 5353.075\tEntropy: 0.828\n",
      "Epoch 163/300:\tReward: -57.6\tLoss: 9490.683\tEntropy: 0.769\n",
      "Epoch 164/300:\tReward: -73.0\tLoss: 4453.220\tEntropy: 0.818\n",
      "Epoch 165/300:\tReward: -62.0\tLoss: 8621.002\tEntropy: 0.810\n",
      "Epoch 166/300:\tReward: -64.4\tLoss: 5722.817\tEntropy: 0.775\n",
      "Epoch 167/300:\tReward: -77.1\tLoss: 8215.696\tEntropy: 0.807\n",
      "Epoch 168/300:\tReward: -67.1\tLoss: 5373.251\tEntropy: 0.777\n",
      "Epoch 169/300:\tReward: -53.5\tLoss: 9526.851\tEntropy: 0.736\n",
      "Epoch 170/300:\tReward: -62.9\tLoss: 5726.659\tEntropy: 0.662\n",
      "Epoch 171/300:\tReward: -77.3\tLoss: 8132.194\tEntropy: 0.769\n",
      "Epoch 172/300:\tReward: -71.6\tLoss: 4642.273\tEntropy: 0.688\n",
      "Epoch 173/300:\tReward: -51.8\tLoss: 9325.770\tEntropy: 0.750\n",
      "Epoch 174/300:\tReward: -60.9\tLoss: 6045.061\tEntropy: 0.621\n",
      "Epoch 175/300:\tReward: -55.8\tLoss: 9241.096\tEntropy: 0.754\n",
      "Epoch 176/300:\tReward: -74.0\tLoss: 4168.857\tEntropy: 0.658\n",
      "Epoch 177/300:\tReward: -55.0\tLoss: 9287.241\tEntropy: 0.720\n",
      "Epoch 178/300:\tReward: -66.0\tLoss: 5416.473\tEntropy: 0.572\n",
      "Epoch 179/300:\tReward: -53.9\tLoss: 9688.553\tEntropy: 0.687\n",
      "Epoch 180/300:\tReward: -75.4\tLoss: 4337.371\tEntropy: 0.553\n",
      "\n",
      "EVALUATION\n",
      "Regex: 22222\n",
      "Total reward: -404.8\n",
      "\n",
      "Epoch 181/300:\tReward: -48.8\tLoss: 9318.465\tEntropy: 0.707\n",
      "Epoch 182/300:\tReward: -51.5\tLoss: 6972.137\tEntropy: 0.571\n",
      "Epoch 183/300:\tReward: -51.4\tLoss: 9429.664\tEntropy: 0.758\n",
      "Epoch 184/300:\tReward: -68.6\tLoss: 4868.870\tEntropy: 0.626\n",
      "Epoch 185/300:\tReward: -46.0\tLoss: 9375.660\tEntropy: 0.673\n",
      "Epoch 186/300:\tReward: -49.1\tLoss: 7721.759\tEntropy: 0.544\n",
      "Epoch 187/300:\tReward: -48.2\tLoss: 9391.955\tEntropy: 0.673\n",
      "Epoch 188/300:\tReward: -59.8\tLoss: 5970.660\tEntropy: 0.466\n",
      "Epoch 189/300:\tReward: -55.8\tLoss: 9811.813\tEntropy: 0.670\n",
      "Epoch 190/300:\tReward: -75.6\tLoss: 3853.972\tEntropy: 0.466\n",
      "Epoch 191/300:\tReward: -45.5\tLoss: 9424.186\tEntropy: 0.659\n",
      "Epoch 192/300:\tReward: -46.4\tLoss: 7690.535\tEntropy: 0.526\n"
     ]
    }
   ],
   "source": [
    "train_eval_loop(\n",
    "    a2c_net,\n",
    "    a2c_optimizer,\n",
    "    agent,\n",
    "    buffer,\n",
    "    epochs=300,\n",
    "    episodes=1000,\n",
    "    eval_period=20,\n",
    "    entropy_beta=0.01,\n",
    "    # entropy_beta=100,\n",
    "    clip_grad=10,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
