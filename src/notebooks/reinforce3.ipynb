{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acebe6ac",
   "metadata": {},
   "source": [
    "# REINFORCE Algorithm test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "36a63ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c6452252",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import warnings\n",
    "from itertools import accumulate\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "ROOT_FOLDER = os.path.join(\".\", \"..\")\n",
    "if ROOT_FOLDER not in sys.path:\n",
    "    sys.path.insert(0, ROOT_FOLDER)\n",
    "\n",
    "\n",
    "from dataset import RegexDataset\n",
    "from environment_metrics import Environment, EnvSettings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.disable(logging.WARNING)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90716482",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "df813811",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int = 420):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8a90f2",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "922307e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('a2d', [0, 1, 0], 1)\n",
      "('2bb', [1, 0, 0], 1)\n",
      "('a2d', [0, 1, 0], 1)\n",
      "('2bb', [1, 0, 0], 1)\n",
      "('2bb', [1, 0, 0], 1)\n",
      "('a2d', [0, 1, 0], 1)\n",
      "('2bb', [1, 0, 0], 1)\n",
      "('a2d', [0, 1, 0], 1)\n",
      "('a2d', [0, 1, 0], 1)\n",
      "('2bb', [1, 0, 0], 1)\n"
     ]
    }
   ],
   "source": [
    "dataset = RegexDataset([\"a2d\", \"2bb\"], r\"\\d+\")\n",
    "data_iter = dataset.create_iterator()\n",
    "\n",
    "for i in range(10):\n",
    "    print(next(data_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb28738e",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "83bb3ce4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = Environment(dataset, settings=EnvSettings(max_steps=10))\n",
    "\n",
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3967b5d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.state_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a7dd245d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action=8\n",
      "(array([0.27272727, 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ]), 0, False)\n",
      "action=6\n",
      "(array([0.27272727, 0.45454545, 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ]), 0, False)\n",
      "action=3\n",
      "(array([0.27272727, 0.45454545, 0.72727273, 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ]), 0, False)\n",
      "action=9\n",
      "(array([0.27272727, 0.45454545, 0.72727273, 0.18181818, 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ]), 0, False)\n",
      "action=10\n",
      "(array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), -225.0, True)\n",
      "action=5\n",
      "(array([0.54545455, 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ]), 0, False)\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "for _ in range(6):\n",
    "    action = np.random.randint(env.action_space)\n",
    "    print(f\"{action=}\")\n",
    "    print(env.step(action))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b87793",
   "metadata": {},
   "source": [
    "## REINFORCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "053f7cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_qvals(\n",
    "    rewards: list[float] | np.ndarray, gamma: float = 0.9, reward_steps: int = 0\n",
    ") -> np.ndarray:\n",
    "    rw_steps = reward_steps if reward_steps != 0 else len(rewards)\n",
    "\n",
    "    return np.array(\n",
    "        [\n",
    "            list(\n",
    "                accumulate(\n",
    "                    reversed(rewards[i : i + rw_steps]), lambda x, y: gamma * x + y\n",
    "                )\n",
    "            )[-1]\n",
    "            for i in range(len(rewards))\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b5d652a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PGN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int = env.state_space,\n",
    "        output_dim: int = env.action_space,\n",
    "        hidden_dim: int = 64,\n",
    "    ) -> None:\n",
    "        super(PGN, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, output_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7341ee6f",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dfd5051c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def choose_action(self, action_logits: torch.Tensor):\n",
    "        return np.random.choice(\n",
    "            range(len(action_logits)), size=1, p=F.softmax(action_logits, dim=0).numpy()\n",
    "        )[0]\n",
    "    def choose_random(self, action_logits):\n",
    "        return np.random.choice(range(len(action_logits)))\n",
    "\n",
    "    def choose_optimal_action(self, action_logits: torch.Tensor) -> int:\n",
    "        return int(np.argmax(F.softmax(action_logits, dim=0).cpu()).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bcb88c",
   "metadata": {},
   "source": [
    "## Trajectory Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "40a580c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrajectoryBuffer:\n",
    "    \"\"\"\n",
    "    Buffer class to store the experience from a unique policy\n",
    "    \"\"\"\n",
    "\n",
    "    def _batch(self, iterable):\n",
    "        ln = len(iterable)\n",
    "        for ndx in range(0, ln, self.batch_size):\n",
    "            yield iterable[ndx : min(ndx + self.batch_size, ln)]\n",
    "\n",
    "    def __init__(self, batch_size: int = 64):\n",
    "        self.batch_size = batch_size\n",
    "        self.clean()\n",
    "\n",
    "    def clean(self):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.discounted_rewards = []\n",
    "\n",
    "    def store(\n",
    "        self,\n",
    "        states_trajectory: np.ndarray,\n",
    "        trajectory: np.ndarray,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Add trajectory values to the buffers and compute the advantage and reward to go\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        states_trajectory:  list that contains states\n",
    "        trajectory: list where each element is a list that contains: reward, action\n",
    "        \"\"\"\n",
    "        assert len(states_trajectory) == len(trajectory)\n",
    "\n",
    "        if len(states_trajectory) > 0:\n",
    "            self.states.extend(states_trajectory)\n",
    "            self.rewards.extend(trajectory[:, 0])\n",
    "            self.actions.extend(trajectory[:, 1])\n",
    "\n",
    "            self.discounted_rewards.extend(calculate_qvals(trajectory[:, 0]))\n",
    "\n",
    "    def get_batches(self, mean_baseline: bool):\n",
    "        if mean_baseline:\n",
    "            mean_rewards = np.mean(self.discounted_rewards)\n",
    "        else:\n",
    "            mean_rewards = 0\n",
    "\n",
    "        for states_batch, actions_batch, discounted_rewards_batch in zip(\n",
    "            self._batch(self.states),\n",
    "            self._batch(self.actions),\n",
    "            self._batch(self.discounted_rewards),\n",
    "        ):\n",
    "            yield (\n",
    "                torch.tensor(states_batch, dtype=torch.float32, device=DEVICE),\n",
    "                torch.tensor(actions_batch, dtype=torch.long, device=DEVICE),\n",
    "                torch.tensor(\n",
    "                    np.array(discounted_rewards_batch) - mean_rewards,\n",
    "                    dtype=torch.float,\n",
    "                    device=DEVICE,\n",
    "                ),\n",
    "            )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.states)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d81125",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c3411e",
   "metadata": {},
   "source": [
    "TBD: generate one Regex and test it on dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0f5e32be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(\n",
    "    pgn_net: nn.Module,\n",
    "    env: Environment,\n",
    "    agent: Agent,\n",
    "    verbose: bool = True,\n",
    ") -> tuple[str, float]:\n",
    "\n",
    "    pgn_net.eval()\n",
    "    max_steps =  env.settings.max_steps\n",
    "    state = env.reset()\n",
    "    regex_actions = []\n",
    "    total_reward = 0.0\n",
    "    done = False\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_steps):\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32, device=DEVICE)\n",
    "            action_logits = pgn_net(state_tensor)\n",
    "            \n",
    "           \n",
    "            action = agent.choose_optimal_action(action_logits)\n",
    "            regex_actions.append(env.idx_to_action(action))\n",
    "            \n",
    "           \n",
    "            next_state, reward, done = env.step(action)\n",
    "            total_reward += reward\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "            state = next_state\n",
    "\n",
    "    if regex_actions and regex_actions[-1] == env._finish_action:\n",
    "        regex_actions = regex_actions[:-1]\n",
    "    \n",
    "    try:\n",
    "        regex = env.rpn.to_infix(regex_actions)\n",
    "    except Exception as e:\n",
    "        regex = f\"Invalid: {regex_actions}\"\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Generated regex: {regex}\")\n",
    "        print(f\"Total reward: {total_reward:.2f}\")\n",
    "    \n",
    "    return regex, total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15ed7e2",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d33e88c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(\n",
    "    pgn_net: nn.Module,\n",
    "    pgn_optimizer: optim.Optimizer,\n",
    "    agent: Agent,\n",
    "    buffer: TrajectoryBuffer,\n",
    "    epochs: int,\n",
    "    episodes: int,\n",
    "    mean_baseline: bool = True,\n",
    "    entropy_beta: float = 0.6,\n",
    "):\n",
    "    set_seed()\n",
    "    \n",
    "    pgn_net.train()\n",
    "    for i in range(1, epochs + 1):\n",
    "        buffer.clean()\n",
    "        state = env.reset()\n",
    "        done_episodes = 0\n",
    "        ep_states_buf, ep_rew_act_buf = [], []\n",
    "        reg_exp = []\n",
    "        train_rewards = []\n",
    "\n",
    "        epoch_loop = tqdm(total=episodes, desc=f\"Epoch #{i}\", position=0, disable=True)\n",
    "\n",
    "        while done_episodes < episodes:\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                action_logits = pgn_net(state_tensor)\n",
    "\n",
    "            # if i < epochs // 5:\n",
    "            #     action = agent.choose_random(action_logits)\n",
    "            # else:\n",
    "            action = agent.choose_action(action_logits)\n",
    "                \n",
    "            \n",
    "            next_state, reward, done = env.step(action)\n",
    "            \n",
    "            reg_exp.append(env.idx_to_action(action))\n",
    "\n",
    "            ep_states_buf.append(state)\n",
    "            ep_rew_act_buf.append([reward, int(action)])\n",
    "\n",
    "            state = next_state\n",
    "            if done:\n",
    "                buffer.store(\n",
    "                    np.array(ep_states_buf),\n",
    "                    np.array(ep_rew_act_buf),\n",
    "                )\n",
    "\n",
    "                ep_states_buf, ep_rew_act_buf = [], []\n",
    "\n",
    "                train_rewards.append(reward)\n",
    "\n",
    "                done_episodes += 1\n",
    "                epoch_loop.update(1)\n",
    "\n",
    "        losses = []\n",
    "        for batch in buffer.get_batches(mean_baseline):\n",
    "            pgn_optimizer.zero_grad()\n",
    "            (\n",
    "                state_batch,\n",
    "                action_batch,\n",
    "                reward_batch,\n",
    "            ) = batch\n",
    "           \n",
    "\n",
    "            logits_v = pgn_net(state_batch)\n",
    "\n",
    "            mx = torch.max(torch.abs(logits_v))\n",
    "            t = 1/i * mx*10\n",
    "            \n",
    "            log_prob_v = F.log_softmax(logits_v / t, dim=1)\n",
    "\n",
    "            log_prob_actions_v = (\n",
    "                reward_batch * log_prob_v[range(len(action_batch)), action_batch]\n",
    "            )\n",
    "            loss_policy_v = -log_prob_actions_v.mean()\n",
    "\n",
    "            prob_v = F.softmax(logits_v, dim=1)\n",
    "            entropy_v = (prob_v * log_prob_v).sum(dim=1).mean()\n",
    "            entropy_loss_v = entropy_beta * entropy_v\n",
    "            loss_v = loss_policy_v + entropy_loss_v\n",
    "\n",
    "            loss_v.backward()\n",
    "\n",
    "            pgn_optimizer.step()\n",
    "\n",
    "            losses.append(loss_v.item())\n",
    "            # return\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {i: >3}/{epochs}:\\tMean reward: {np.mean(train_rewards):.3f}\\tMean Loss: {np.mean(losses):.3f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "af9b2ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()\n",
    "env = Environment(\n",
    "    RegexDataset([\"a2d\", \"2bb\", \"cc2\", \"b6J\", \"G1S\", \"pp3\", \"km9\", \"mb4\", \"o70\", \"pi5\"], r\"\\d+\"), settings=EnvSettings(max_steps=10)\n",
    ")\n",
    "\n",
    "agent = Agent()\n",
    "buffer = TrajectoryBuffer(batch_size=32)\n",
    "\n",
    "pgn_net = PGN().to(DEVICE)\n",
    "# pgn_optimizer = optim.SGD(pgn_net.parameters(), lr=1e-3, nesterov=True, momentum=0.99)\n",
    "pgn_optimizer = optim.Adam(pgn_net.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5f287c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/20:\tMean reward: -289.910\tMean Loss: -1.559\n",
      "Epoch   2/20:\tMean reward: -252.800\tMean Loss: -5.864\n",
      "Epoch   3/20:\tMean reward: -27.240\tMean Loss: -0.896\n",
      "Epoch   4/20:\tMean reward: -26.050\tMean Loss: -1.440\n",
      "Epoch   5/20:\tMean reward: -343.030\tMean Loss: -37.497\n",
      "Epoch   6/20:\tMean reward: -369.475\tMean Loss: -58.492\n",
      "Epoch   7/20:\tMean reward: -338.175\tMean Loss: -56.140\n",
      "Epoch   8/20:\tMean reward: -375.000\tMean Loss: -73.897\n",
      "Epoch   9/20:\tMean reward: -363.025\tMean Loss: -68.925\n",
      "Epoch  10/20:\tMean reward: -375.000\tMean Loss: -89.170\n",
      "Epoch  11/20:\tMean reward: -375.000\tMean Loss: -93.330\n",
      "Epoch  12/20:\tMean reward: -250.175\tMean Loss: -22.738\n",
      "Epoch  13/20:\tMean reward: -375.000\tMean Loss: -93.738\n",
      "Epoch  14/20:\tMean reward: -375.000\tMean Loss: -106.856\n",
      "Epoch  15/20:\tMean reward: -375.000\tMean Loss: -85.721\n",
      "Epoch  16/20:\tMean reward: -251.200\tMean Loss: -33.859\n",
      "Epoch  17/20:\tMean reward: -375.000\tMean Loss: -112.499\n",
      "Epoch  18/20:\tMean reward: -375.000\tMean Loss: -113.119\n",
      "Epoch  19/20:\tMean reward: -375.000\tMean Loss: -140.729\n",
      "Epoch  20/20:\tMean reward: -373.825\tMean Loss: -127.095\n"
     ]
    }
   ],
   "source": [
    "train_loop(\n",
    "    pgn_net, pgn_optimizer, agent, buffer, mean_baseline=True, epochs=20, episodes=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bf87e38f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated regex: 2222266555\n",
      "Total reward: -375.00\n",
      "Final regex: 2222266555\n"
     ]
    }
   ],
   "source": [
    "env_eval = Environment(\n",
    "    RegexDataset([\"a2365d\"], r\"\\d+\"),  \n",
    "    settings=EnvSettings( max_steps  = 10, full_match_bonus=100)\n",
    ")\n",
    "\n",
    "best_regex, reward = evaluate(pgn_net, env_eval, agent)\n",
    "print(f\"Final regex: {best_regex}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309183b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
