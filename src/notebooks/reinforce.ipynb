{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acebe6ac",
   "metadata": {},
   "source": [
    "# REINFORCE Algorithm test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1023,
   "id": "36a63ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1024,
   "id": "c6452252",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 1024,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import warnings\n",
    "from itertools import accumulate\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "ROOT_FOLDER = os.path.join(\".\", \"..\")\n",
    "if ROOT_FOLDER not in sys.path:\n",
    "    sys.path.insert(0, ROOT_FOLDER)\n",
    "\n",
    "\n",
    "from dataset import RegexDataset\n",
    "from environment import Environment, EnvSettings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.disable(logging.WARNING)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90716482",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1025,
   "id": "df813811",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int = 420):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8a90f2",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1026,
   "id": "922307e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('a2d', [0, 1, 0], 1)\n",
      "('2bb', [1, 0, 0], 1)\n",
      "('a2d', [0, 1, 0], 1)\n",
      "('2bb', [1, 0, 0], 1)\n",
      "('2bb', [1, 0, 0], 1)\n",
      "('a2d', [0, 1, 0], 1)\n",
      "('2bb', [1, 0, 0], 1)\n",
      "('a2d', [0, 1, 0], 1)\n",
      "('a2d', [0, 1, 0], 1)\n",
      "('2bb', [1, 0, 0], 1)\n"
     ]
    }
   ],
   "source": [
    "dataset = RegexDataset([\"a2d\", \"2bb\"], r\"\\d+\")\n",
    "data_iter = dataset.create_iterator()\n",
    "\n",
    "for i in range(10):\n",
    "    print(next(data_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb28738e",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1027,
   "id": "83bb3ce4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 1027,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = Environment(dataset, settings=EnvSettings(max_steps=3))\n",
    "\n",
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1028,
   "id": "a7dd245d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action=8\n",
      "(array([0.27272727, 0.        , 0.        ]), 0, False)\n",
      "action=6\n",
      "(array([0.27272727, 0.45454545, 0.        ]), 0, False)\n",
      "action=3\n",
      "(array([0., 0., 0.]), -10104.0, True)\n",
      "action=5\n",
      "(array([0.54545455, 0.        , 0.        ]), 0, False)\n",
      "action=7\n",
      "(array([0.54545455, 0.36363636, 0.        ]), 0, False)\n",
      "action=3\n",
      "(array([0., 0., 0.]), -10104.0, True)\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "for _ in range(6):\n",
    "    action = np.random.randint(env.action_space)\n",
    "    print(f\"{action=}\")\n",
    "    print(env.step(action))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b87793",
   "metadata": {},
   "source": [
    "## REINFORCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1038,
   "id": "053f7cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_qvals(\n",
    "    rewards: list[float] | np.ndarray, gamma: float = 1.0, reward_steps: int = 0\n",
    ") -> np.ndarray:\n",
    "    rw_steps = reward_steps if reward_steps != 0 else len(rewards)\n",
    "\n",
    "    return np.array(\n",
    "        [\n",
    "            list(\n",
    "                accumulate(\n",
    "                    reversed(rewards[i : i + rw_steps]), lambda x, y: gamma * x + y\n",
    "                )\n",
    "            )[-1]\n",
    "            for i in range(len(rewards))\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1030,
   "id": "b5d652a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PGN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int = env.state_space,\n",
    "        output_dim: int = env.action_space,\n",
    "        hidden_dim: int = 64,\n",
    "    ) -> None:\n",
    "        super(PGN, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(16, output_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7341ee6f",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1031,
   "id": "dfd5051c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def choose_action(self, action_logits: torch.Tensor):\n",
    "        return np.random.choice(\n",
    "            range(len(action_logits)), size=1, p=F.softmax(action_logits, dim=0).numpy()\n",
    "        )[0]\n",
    "\n",
    "    def choose_optimal_action(self, action_logits: torch.Tensor) -> int:\n",
    "        return int(np.argmax(F.softmax(action_logits, dim=0).cpu()).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bcb88c",
   "metadata": {},
   "source": [
    "## Trajectory Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a580c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrajectoryBuffer:\n",
    "    \"\"\"\n",
    "    Buffer class to store the experience from a unique policy\n",
    "    \"\"\"\n",
    "\n",
    "    def _batch(self, iterable):\n",
    "        ln = len(iterable)\n",
    "        for ndx in range(0, ln, self.batch_size):\n",
    "            yield iterable[ndx : min(ndx + self.batch_size, ln)]\n",
    "\n",
    "    def __init__(self, batch_size: int = 64):\n",
    "        self.batch_size = batch_size\n",
    "        self.clean()\n",
    "\n",
    "    def clean(self):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.discounted_rewards = []\n",
    "\n",
    "    def store(\n",
    "        self,\n",
    "        states_trajectory: np.ndarray,\n",
    "        trajectory: np.ndarray,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Add trajectory values to the buffers and compute the advantage and reward to go\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        states_trajectory:  list that contains states\n",
    "        trajectory: list where each element is a list that contains: reward, action\n",
    "        \"\"\"\n",
    "        assert len(states_trajectory) == len(trajectory)\n",
    "\n",
    "        if len(states_trajectory) > 0:\n",
    "            self.states.extend(states_trajectory)\n",
    "            self.actions.extend(trajectory[:, 1])\n",
    "\n",
    "            self.discounted_rewards.extend(calculate_qvals(trajectory[:, 0]))\n",
    "\n",
    "    def get_batches(self, mean_baseline: bool):\n",
    "        if mean_baseline:\n",
    "            mean_rewards = np.mean(self.discounted_rewards)\n",
    "        else:\n",
    "            mean_rewards = 0\n",
    "\n",
    "        for states_batch, actions_batch, discounted_rewards_batch in zip(\n",
    "            self._batch(self.states),\n",
    "            self._batch(self.actions),\n",
    "            self._batch(self.discounted_rewards),\n",
    "        ):\n",
    "            yield (\n",
    "                torch.tensor(states_batch, dtype=torch.float32, device=DEVICE),\n",
    "                torch.tensor(actions_batch, dtype=torch.long, device=DEVICE),\n",
    "                torch.tensor(\n",
    "                    np.array(discounted_rewards_batch) - mean_rewards,\n",
    "                    dtype=torch.float,\n",
    "                    device=DEVICE,\n",
    "                ),\n",
    "            )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.states)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d81125",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c3411e",
   "metadata": {},
   "source": [
    "TBD: generate one Regex and test it on dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15ed7e2",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1033,
   "id": "d33e88c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(\n",
    "    pgn_net: nn.Module,\n",
    "    pgn_optimizer: optim.Optimizer,\n",
    "    agent: Agent,\n",
    "    buffer: TrajectoryBuffer,\n",
    "    epochs: int,\n",
    "    episodes: int,\n",
    "    mean_baseline: bool = True,\n",
    "    entropy_beta: float = 1e-3,\n",
    "):\n",
    "    set_seed()\n",
    "\n",
    "    pgn_net.train()\n",
    "    for i in range(1, epochs + 1):\n",
    "        buffer.clean()\n",
    "        state = env.reset()\n",
    "        done_episodes = 0\n",
    "        ep_states_buf, ep_rew_act_buf = [], []\n",
    "\n",
    "        train_rewards = []\n",
    "\n",
    "        epoch_loop = tqdm(total=episodes, desc=f\"Epoch #{i}\", position=0, disable=True)\n",
    "\n",
    "        while done_episodes < episodes:\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                action_logits = pgn_net(state_tensor)\n",
    "\n",
    "            action = agent.choose_action(action_logits)\n",
    "            next_state, reward, done = env.step(action)\n",
    "\n",
    "            ep_states_buf.append(state)\n",
    "            ep_rew_act_buf.append([reward, int(action)])\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                buffer.store(\n",
    "                    np.array(ep_states_buf),\n",
    "                    np.array(ep_rew_act_buf),\n",
    "                )\n",
    "\n",
    "                ep_states_buf, ep_rew_act_buf = [], []\n",
    "\n",
    "                train_rewards.append(reward)\n",
    "\n",
    "                done_episodes += 1\n",
    "                epoch_loop.update(1)\n",
    "\n",
    "        losses = []\n",
    "        for batch in buffer.get_batches(mean_baseline):\n",
    "            pgn_optimizer.zero_grad()\n",
    "            (\n",
    "                state_batch,\n",
    "                action_batch,\n",
    "                reward_batch,\n",
    "            ) = batch\n",
    "\n",
    "            logits_v = pgn_net(state_batch)\n",
    "            log_prob_v = F.log_softmax(logits_v, dim=1)\n",
    "\n",
    "            log_prob_actions_v = (\n",
    "                reward_batch * log_prob_v[range(len(state_batch)), action_batch]\n",
    "            )\n",
    "            loss_policy_v = -log_prob_actions_v.mean()\n",
    "\n",
    "            prob_v = F.softmax(logits_v, dim=1)\n",
    "            entropy_v = (prob_v * log_prob_v).sum(dim=1).mean()\n",
    "            entropy_loss_v = entropy_beta * entropy_v\n",
    "            loss_v = loss_policy_v + entropy_loss_v\n",
    "\n",
    "            loss_v.backward()\n",
    "\n",
    "            pgn_optimizer.step()\n",
    "\n",
    "            losses.append(loss_v.item())\n",
    "            # return\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {i: >3}/{epochs}:\\tMean reward: {np.mean(train_rewards):.3f}\\tMean Loss: {np.mean(losses):.3f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1034,
   "id": "af9b2ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()\n",
    "env = Environment(\n",
    "    RegexDataset([\"a2d\", \"2bb\", \"cc2\"], r\"\\d+\"), settings=EnvSettings(max_steps=3)\n",
    ")\n",
    "\n",
    "agent = Agent()\n",
    "buffer = TrajectoryBuffer(batch_size=1)\n",
    "\n",
    "pgn_net = PGN().to(DEVICE)\n",
    "# pgn_optimizer = optim.SGD(pgn_net.parameters(), lr=1e-3, nesterov=True, momentum=0.99)\n",
    "pgn_optimizer = optim.Adam(pgn_net.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1035,
   "id": "5f287c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/20:\tMean reward: -11583.139\tMean Loss: -4358.246\n",
      "Epoch   2/20:\tMean reward: -10103.874\tMean Loss: -0.005\n",
      "Epoch   3/20:\tMean reward: -10103.818\tMean Loss: -0.079\n",
      "Epoch   4/20:\tMean reward: -10103.186\tMean Loss: -1.620\n",
      "Epoch   5/20:\tMean reward: -9535.390\tMean Loss: -3491.617\n",
      "Epoch   6/20:\tMean reward: -29859.216\tMean Loss: 689.568\n",
      "Epoch   7/20:\tMean reward: -22742.263\tMean Loss: -137014.405\n",
      "Epoch   8/20:\tMean reward: -1456.564\tMean Loss: -20592.165\n",
      "Epoch   9/20:\tMean reward: -9942.359\tMean Loss: 127.021\n",
      "Epoch  10/20:\tMean reward: -6810.539\tMean Loss: -32750.277\n",
      "Epoch  11/20:\tMean reward: -2.000\tMean Loss: -0.000\n",
      "Epoch  12/20:\tMean reward: -2.000\tMean Loss: -0.000\n",
      "Epoch  13/20:\tMean reward: -2.000\tMean Loss: -0.000\n",
      "Epoch  14/20:\tMean reward: -2.000\tMean Loss: -0.000\n",
      "Epoch  15/20:\tMean reward: -2.000\tMean Loss: -0.000\n",
      "Epoch  16/20:\tMean reward: -2.000\tMean Loss: -0.000\n",
      "Epoch  17/20:\tMean reward: -2.000\tMean Loss: -0.000\n",
      "Epoch  18/20:\tMean reward: -2.000\tMean Loss: -0.000\n",
      "Epoch  19/20:\tMean reward: -2.000\tMean Loss: -0.000\n",
      "Epoch  20/20:\tMean reward: -2.000\tMean Loss: -0.000\n"
     ]
    }
   ],
   "source": [
    "train_loop(pgn_net, pgn_optimizer, agent, buffer, epochs=20, episodes=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1036,
   "id": "bc3fd856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = Environment(RegexDataset([\"a2d\", \"2bb\"], r\"\\d+\"), settings=EnvSettings(max_steps=3))\n",
    "\n",
    "# state = env.reset()\n",
    "# a1 = env.action_to_idx(\"2\")\n",
    "# a2 = env.action_to_idx(\"FIN\")\n",
    "# print(env.step(a1))\n",
    "# print(env.step(a2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
