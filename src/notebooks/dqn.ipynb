{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acebe6ac",
   "metadata": {},
   "source": [
    "# DQN Algorithm test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "36a63ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6452252",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "ROOT_FOLDER = os.path.join(\".\", \"..\")\n",
    "if ROOT_FOLDER not in sys.path:\n",
    "    sys.path.insert(0, ROOT_FOLDER)\n",
    "\n",
    "\n",
    "from dataset import RegexDataset\n",
    "from environment import Environment, EnvSettings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.disable(logging.WARNING)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90716482",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "df813811",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int = 420):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8a90f2",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "922307e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('a2d', [0, 1, 0], 1)\n",
      "('2bb', [1, 0, 0], 1)\n",
      "('a2d', [0, 1, 0], 1)\n",
      "('2bb', [1, 0, 0], 1)\n",
      "('2bb', [1, 0, 0], 1)\n",
      "('a2d', [0, 1, 0], 1)\n",
      "('2bb', [1, 0, 0], 1)\n",
      "('a2d', [0, 1, 0], 1)\n",
      "('a2d', [0, 1, 0], 1)\n",
      "('2bb', [1, 0, 0], 1)\n"
     ]
    }
   ],
   "source": [
    "dataset = RegexDataset([\"a2d\", \"2bb\"], r\"\\d+\")\n",
    "data_iter = dataset.create_iterator()\n",
    "\n",
    "for i in range(10):\n",
    "    print(next(data_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb28738e",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "83bb3ce4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = Environment(dataset, settings=EnvSettings(max_steps=3))\n",
    "\n",
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "a7dd245d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action=72\n",
      "(array([0.28712871, 0.        , 0.        ]), 0, False)\n",
      "action=6\n",
      "(array([0.28712871, 0.94059406, 0.        ]), 0, False)\n",
      "action=63\n",
      "(array([0., 0., 0.]), -100000, True)\n",
      "action=53\n",
      "(array([0.47524752, 0.        , 0.        ]), 0, False)\n",
      "action=19\n",
      "(array([0.47524752, 0.81188119, 0.        ]), 0, False)\n",
      "action=81\n",
      "(array([0., 0., 0.]), -10104.0, True)\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "for _ in range(6):\n",
    "    action = np.random.randint(env.action_space)\n",
    "    print(f\"{action=}\")\n",
    "    print(env.step(action))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b87793",
   "metadata": {},
   "source": [
    "## DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "b5d652a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int = env.state_space,\n",
    "        output_dim: int = env.action_space,\n",
    "        hidden_dim: int = 32,\n",
    "    ) -> None:\n",
    "        super(DQN, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7341ee6f",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "dfd5051c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, action_space: int) -> None:\n",
    "        self.action_space = action_space\n",
    "\n",
    "    def sample_action(self) -> int:\n",
    "        return np.random.choice(self.action_space)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def choose_optimal_action(self, state: torch.Tensor, dqn: nn.Module) -> int:\n",
    "        q_vals_v = dqn(state)\n",
    "        act_v = torch.argmax(q_vals_v)\n",
    "        return int(act_v.item())\n",
    "\n",
    "    def choose_action(self, state: torch.Tensor, dqn: nn.Module, epsilon: float) -> int:\n",
    "        if np.random.random() < epsilon:\n",
    "            return self.sample_action()\n",
    "        return self.choose_optimal_action(state, dqn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bcb88c",
   "metadata": {},
   "source": [
    "## Trajectory Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "40a580c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrajectoryBuffer:\n",
    "    \"\"\"\n",
    "    Buffer class to store the experience from a unique policy\n",
    "    \"\"\"\n",
    "\n",
    "    def _batch(self, iterable, n=1):\n",
    "        ln = len(iterable)\n",
    "        for ndx in range(0, ln, n):\n",
    "            yield iterable[ndx : min(ndx + n, ln)]\n",
    "\n",
    "    def __init__(self, batch_size: int = 64):\n",
    "        self.batch_size = 64\n",
    "        self.clean()\n",
    "\n",
    "    def clean(self):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "        self.next_states = []\n",
    "\n",
    "    def store(\n",
    "        self,\n",
    "        state: np.ndarray,\n",
    "        action: int,\n",
    "        reward: float,\n",
    "        done: bool,\n",
    "        next_state: np.ndarray,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Add trajectory values to the buffers\n",
    "        \"\"\"\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.rewards.append(reward)\n",
    "        self.dones.append(int(done))\n",
    "        self.next_states.append(next_state)\n",
    "\n",
    "    def get_batches(self):\n",
    "        for (\n",
    "            states_batch,\n",
    "            actions_batch,\n",
    "            rewards_batch,\n",
    "            dones_batch,\n",
    "            nest_states_batch,\n",
    "        ) in zip(\n",
    "            self._batch(self.states),\n",
    "            self._batch(self.actions),\n",
    "            self._batch(self.rewards),\n",
    "            self._batch(self.dones),\n",
    "            self._batch(self.next_states),\n",
    "        ):\n",
    "            yield (\n",
    "                torch.tensor(states_batch, dtype=torch.float32, device=DEVICE),\n",
    "                torch.tensor(actions_batch, dtype=torch.long, device=DEVICE),\n",
    "                torch.tensor(rewards_batch, dtype=torch.float, device=DEVICE),\n",
    "                torch.tensor(dones_batch, dtype=torch.bool, device=DEVICE),\n",
    "                torch.tensor(nest_states_batch, dtype=torch.float, device=DEVICE),\n",
    "            )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.states)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d81125",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c3411e",
   "metadata": {},
   "source": [
    "TBD: generate one Regex and test it on dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15ed7e2",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "d33e88c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(\n",
    "    dqn_net: nn.Module,\n",
    "    dqn_target_net: nn.Module,\n",
    "    dqn_optimizer: optim.Optimizer,\n",
    "    agent: Agent,\n",
    "    buffer: TrajectoryBuffer,\n",
    "    epochs: int,\n",
    "    episodes: int,\n",
    "    dqn_sync_period: int = 1,\n",
    "    gamma: float = 0.99,\n",
    "):\n",
    "    set_seed()\n",
    "    buffer.clean()\n",
    "    dqn_net.train()\n",
    "\n",
    "    epsilon = 0.99\n",
    "    epsilon_decay = 0.99\n",
    "\n",
    "    for i in range(1, epochs + 1):\n",
    "        state = env.reset()\n",
    "        done_episodes = 0\n",
    "\n",
    "        train_rewards = []\n",
    "\n",
    "        epoch_loop = tqdm(total=episodes, desc=f\"Epoch #{i}\", position=0, disable=True)\n",
    "\n",
    "        while done_episodes < episodes:\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32, device=DEVICE)\n",
    "            action = agent.choose_action(state_tensor, dqn_net, epsilon)\n",
    "            next_state, reward, done = env.step(action)\n",
    "\n",
    "            buffer.store(\n",
    "                state,\n",
    "                action,\n",
    "                reward,\n",
    "                done,\n",
    "                next_state,\n",
    "            )\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                train_rewards.append(reward)\n",
    "                state = env.reset()\n",
    "\n",
    "                done_episodes += 1\n",
    "                epoch_loop.update(1)\n",
    "\n",
    "        # Update DQN\n",
    "        if i % dqn_sync_period == 0:\n",
    "            dqn_target_net.load_state_dict(dqn_net.state_dict())\n",
    "            # Should we clear buffer here?\n",
    "            buffer.clean()\n",
    "\n",
    "        for batch in buffer.get_batches():\n",
    "            (\n",
    "                state_batch,\n",
    "                action_batch,\n",
    "                reward_batch,\n",
    "                done_batch,\n",
    "                next_state_batch,\n",
    "            ) = batch\n",
    "\n",
    "            dqn_optimizer.zero_grad()\n",
    "\n",
    "            state_action_values = (\n",
    "                dqn_net(state_batch).gather(1, action_batch.unsqueeze(-1)).squeeze(-1)\n",
    "            )\n",
    "            with torch.no_grad():\n",
    "                next_state_values = dqn_target_net(next_state_batch).max(1)[0]\n",
    "                next_state_values[done_batch] = 0.0\n",
    "                next_state_values = next_state_values.detach()\n",
    "\n",
    "            expected_state_action_values = next_state_values * gamma + reward_batch\n",
    "            loss_v = F.mse_loss(state_action_values, expected_state_action_values)\n",
    "            loss_v.backward()\n",
    "\n",
    "            dqn_optimizer.step()\n",
    "\n",
    "        epsilon *= epsilon_decay\n",
    "        print(f\"Epoch {i: >3}/{epochs}:\\tMean reward: {np.mean(train_rewards)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "af9b2ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()\n",
    "env = Environment(RegexDataset([\"a2d\", \"2bb\"], r\"\\d+\"), settings=EnvSettings(max_steps=3))\n",
    "\n",
    "agent = Agent(env.action_space)\n",
    "buffer = TrajectoryBuffer()\n",
    "\n",
    "dqn_net = DQN().to(DEVICE)\n",
    "dqn_target_net = DQN()\n",
    "dqn_optimizer = optim.Adam(dqn_net.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "5f287c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/50:\tMean reward: -22461.08\n",
      "Epoch   2/50:\tMean reward: -23100.453\n",
      "Epoch   3/50:\tMean reward: -22250.596\n",
      "Epoch   4/50:\tMean reward: -21970.906\n",
      "Epoch   5/50:\tMean reward: -21352.037\n",
      "Epoch   6/50:\tMean reward: -21571.445\n",
      "Epoch   7/50:\tMean reward: -21852.127\n",
      "Epoch   8/50:\tMean reward: -23229.94\n",
      "Epoch   9/50:\tMean reward: -23509.543\n",
      "Epoch  10/50:\tMean reward: -22161.096\n",
      "Epoch  11/50:\tMean reward: -22340.994\n",
      "Epoch  12/50:\tMean reward: -22580.69\n",
      "Epoch  13/50:\tMean reward: -24049.219\n",
      "Epoch  14/50:\tMean reward: -21342.738\n",
      "Epoch  15/50:\tMean reward: -23409.647\n",
      "Epoch  16/50:\tMean reward: -22500.483\n",
      "Epoch  17/50:\tMean reward: -23418.859\n",
      "Epoch  18/50:\tMean reward: -23429.545\n",
      "Epoch  19/50:\tMean reward: -21472.156\n",
      "Epoch  20/50:\tMean reward: -23429.753\n",
      "Epoch  21/50:\tMean reward: -21041.955\n",
      "Epoch  22/50:\tMean reward: -22739.79\n",
      "Epoch  23/50:\tMean reward: -22091.216\n",
      "Epoch  24/50:\tMean reward: -22670.39\n",
      "Epoch  25/50:\tMean reward: -22670.203\n",
      "Epoch  26/50:\tMean reward: -22889.81\n",
      "Epoch  27/50:\tMean reward: -24088.935\n",
      "Epoch  28/50:\tMean reward: -23958.922\n",
      "Epoch  29/50:\tMean reward: -22770.885\n",
      "Epoch  30/50:\tMean reward: -22540.312\n",
      "Epoch  31/50:\tMean reward: -22211.398\n",
      "Epoch  32/50:\tMean reward: -23400.056\n",
      "Epoch  33/50:\tMean reward: -24078.733\n",
      "Epoch  34/50:\tMean reward: -23978.425\n",
      "Epoch  35/50:\tMean reward: -24347.806\n",
      "Epoch  36/50:\tMean reward: -25117.791\n",
      "Epoch  37/50:\tMean reward: -25686.677\n",
      "Epoch  38/50:\tMean reward: -25247.281\n",
      "Epoch  39/50:\tMean reward: -25856.371\n",
      "Epoch  40/50:\tMean reward: -25317.378\n",
      "Epoch  41/50:\tMean reward: -24487.73\n",
      "Epoch  42/50:\tMean reward: -23389.755\n",
      "Epoch  43/50:\tMean reward: -25476.681\n",
      "Epoch  44/50:\tMean reward: -26245.741\n",
      "Epoch  45/50:\tMean reward: -27245.29\n",
      "Epoch  46/50:\tMean reward: -25536.782\n",
      "Epoch  47/50:\tMean reward: -25747.062\n",
      "Epoch  48/50:\tMean reward: -28873.516\n",
      "Epoch  49/50:\tMean reward: -27584.877\n",
      "Epoch  50/50:\tMean reward: -26176.339\n"
     ]
    }
   ],
   "source": [
    "train_loop(\n",
    "    dqn_net, dqn_target_net, dqn_optimizer, agent, buffer, epochs=50, episodes=1000\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
