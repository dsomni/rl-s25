{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703802b3",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Callable\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ac21ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T21:27:55.999150300Z",
     "start_time": "2025-04-16T20:10:40.442306Z"
    }
   },
   "outputs": [],
   "source": [
    "def unary_lambda(symbol: str) -> Callable[[str], str]:\n",
    "    return lambda a: f\"({a}){symbol}\"\n",
    "\n",
    "\n",
    "letters = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "digits = \"0123456789\"\n",
    "\n",
    "quantifiers: set[str] = {\"*\", \"+\", \"?\"}\n",
    "\n",
    "unary_regex: dict[str, Callable[[str], str]] = {\n",
    "    **{symbol: unary_lambda(symbol) for symbol in quantifiers},\n",
    "    **{symbol: unary_lambda(f\"{symbol}?\") for symbol in quantifiers},  # greedy versions\n",
    "}\n",
    "binary_regex: dict[str, Callable[[str, str], str]] = {\n",
    "    \"concat\": lambda a, b: a + b,  # concatenation\n",
    "    \"|\": lambda a, b: f\"{a}|{b}\",\n",
    "}\n",
    "many_to_one_regex: dict[str, Callable[[list[str]], str]] = {\n",
    "    \"[]\": lambda a: \"[\" + \"\".join([f\"({x})\" for x in a]) + \"]\",  # set\n",
    "    \"^[]\": lambda a: \"[^\" + \"\".join([f\"({x})\" for x in a]) + \"]\",  # not in set\n",
    "    \"concat_all\": \"\".join,\n",
    "}\n",
    "\n",
    "specials_regex: set[str] = {\n",
    "    \".\",\n",
    "    \"^\",\n",
    "    \"$\",\n",
    "}\n",
    "\n",
    "operands_regex: set[str] = {\n",
    "    *specials_regex,\n",
    "    *[f\"\\\\{s}\" for s in specials_regex],\n",
    "    *letters,\n",
    "    *letters.upper(),\n",
    "    *digits,\n",
    "    *[f\"\\\\{s}\" for s in quantifiers],\n",
    "}\n",
    "\n",
    "\n",
    "def rpn_to_infix_regex(expression: list):\n",
    "    global unary_regex, binary_regex, operands_regex, many_to_one_regex\n",
    "    if len(expression) == 0:\n",
    "        return \"\"\n",
    "\n",
    "    if expression[-1] != \"concat_all\":\n",
    "        expression.append(\"concat_all\")\n",
    "    stack = []\n",
    "\n",
    "    for token in expression:\n",
    "        if token in many_to_one_regex:\n",
    "            stack = [many_to_one_regex[token](list(reversed(stack)))]\n",
    "            continue\n",
    "\n",
    "        if token in binary_regex:\n",
    "            operand2 = stack.pop()\n",
    "            operand1 = stack.pop()\n",
    "            stack.append(binary_regex[token](operand1, operand2))\n",
    "            continue\n",
    "        if token in unary_regex:\n",
    "            operand = stack.pop()\n",
    "            stack.append(f\"({token}{operand})\")\n",
    "            continue\n",
    "        if token in operands_regex:\n",
    "            stack.append(token)\n",
    "            continue\n",
    "        raise RuntimeError(f\"Operand '{token}' is unknown\")\n",
    "\n",
    "    return stack[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91456c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers: set[str] = {\n",
    "    \"1\",\n",
    "    \"2\",\n",
    "    \"3\",\n",
    "    \"4\",\n",
    "    \"5\",\n",
    "    \"6\",\n",
    "    \"7\",\n",
    "    \"8\",\n",
    "    \"9\",\n",
    "    \"0\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a60bc2c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T21:27:56.002150100Z",
     "start_time": "2025-04-16T21:14:52.496199Z"
    }
   },
   "outputs": [],
   "source": [
    "IDX_TO_ACTIONS = {\n",
    "    (i): action\n",
    "    for i, action in enumerate(\n",
    "        set().union(\n",
    "            numbers,\n",
    "            # operands_regex,\n",
    "            # set(binary_regex.keys()),\n",
    "            # set(unary_regex.keys()),\n",
    "            # set(many_to_one_regex.keys()),\n",
    "        )\n",
    "    )\n",
    "}\n",
    "IDX_TO_ACTIONS[len(IDX_TO_ACTIONS)] = \"FINISH\"\n",
    "ACTIONS_TO_IDX = {v: k for k, v in IDX_TO_ACTIONS.items()}\n",
    "\n",
    "ACTIONS = set(ACTIONS_TO_IDX.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718e4442",
   "metadata": {},
   "outputs": [],
   "source": [
    "IDX_TO_ACTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a016775",
   "metadata": {},
   "outputs": [],
   "source": [
    "HIGHEST = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bb8f9c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T21:27:56.003150200Z",
     "start_time": "2025-04-16T21:14:53.449219Z"
    }
   },
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    penalty = -100\n",
    "    # penalty = 1\n",
    "    word_penalty = -10000\n",
    "    len_penalty = -1\n",
    "\n",
    "    max_steps = 5\n",
    "\n",
    "    def __init__(\n",
    "        self, text: str, target: list[int], words_count: int, penalty_weights: dict = None\n",
    "    ):\n",
    "        self.text = text\n",
    "        self.target = target\n",
    "        self.words_count = words_count\n",
    "        self.current_index = 0\n",
    "        self.regexp = None\n",
    "\n",
    "        self.empty_state_idx = len(ACTIONS_TO_IDX)\n",
    "        self.finish_action_idx = ACTIONS_TO_IDX[\"FINISH\"]\n",
    "\n",
    "        default_weights = {\n",
    "            \"f1\": 10.0,\n",
    "            \"precision\": 2.0,\n",
    "            \"recall\": 2.0,\n",
    "            \"complexity\": -0.5,\n",
    "            \"full_match\": 50.0,\n",
    "            \"syntax_error\": -100.0,\n",
    "            \"partial_progress\": 5.0,\n",
    "            \"length_penalty\": -0.3,\n",
    "        }\n",
    "        self.penalty_weights = penalty_weights or default_weights\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.step_idx = 0\n",
    "        self.state = np.array([self.empty_state_idx] * self.max_steps)\n",
    "        self.regex_history = []\n",
    "        return self.get_state()\n",
    "\n",
    "    def get_state(self):\n",
    "        return self.state\n",
    "\n",
    "    def get_state_tensor(self):\n",
    "        return torch.FloatTensor(1 - (self.state / self.empty_state_idx)).unsqueeze(0)\n",
    "\n",
    "    def step(self, action: int) -> tuple[list, float, bool]:\n",
    "        self.step_idx += 1\n",
    "        if action == self.finish_action_idx:\n",
    "            reward = self.reward(self.state[: self.step_idx])\n",
    "            # self.reset()\n",
    "            return self.state, reward, True\n",
    "        if self.step_idx > self.max_steps:  # finish action\n",
    "            reward = self.reward(self.state[: self.step_idx])\n",
    "            # self.reset()\n",
    "            return self.state, reward, True\n",
    "\n",
    "        self.state[self.step_idx - 1] = action\n",
    "        return self.state, self.reward(self.state[: self.step_idx]), False\n",
    "\n",
    "    def _calculate_metrics(self, regex_str: str):\n",
    "        try:\n",
    "            # compiled_re = re.compile(regex_str)\n",
    "            # matches = [m.span() for m in compiled_re.finditer(self.text)]\n",
    "            matches = [m.span() for m in re.finditer(regex_str, self.text)]\n",
    "\n",
    "        except:\n",
    "            print(\"ERROR_calculate_metrics\")\n",
    "            return None, None, None, True\n",
    "\n",
    "        target_mask = self.target\n",
    "        pred_mask = np.zeros_like(target_mask)\n",
    "\n",
    "        # Create prediction mask\n",
    "        for start, end in matches:\n",
    "            pred_mask[start:end] = 1\n",
    "\n",
    "        reverse_target_mask = [1 - x for x in target_mask]\n",
    "        reverse_pred_mask = [1 - x for x in pred_mask]\n",
    "\n",
    "        # Calculate metrics\n",
    "        tp = np.logical_and(pred_mask, target_mask).sum()\n",
    "        fp = np.logical_and(pred_mask, reverse_target_mask).sum()\n",
    "        fn = np.logical_and(reverse_pred_mask, target_mask).sum()\n",
    "\n",
    "        precision = tp / (tp + fp + 1e-9)\n",
    "        recall = tp / (tp + fn + 1e-9)\n",
    "        f1 = 2 * (precision * recall) / (precision + recall + 1e-9)\n",
    "\n",
    "        return f1, precision, recall, False\n",
    "\n",
    "    def reward(self, regex_actions: list[int]):\n",
    "        # Convert actions to regex string\n",
    "        try:\n",
    "            regex_symbols = [\n",
    "                IDX_TO_ACTIONS[x]\n",
    "                for x in regex_actions\n",
    "                if x != self.finish_action_idx or x != self.empty_state_idx\n",
    "            ]\n",
    "            regex_str = rpn_to_infix_regex(regex_symbols)\n",
    "            print(regex_str)\n",
    "            self.regex_history.append(regex_str)\n",
    "        except Exception:\n",
    "            # Invalid regex construction\n",
    "            print(\"ERROR_syntax_error\")\n",
    "            return self.penalty_weights[\"syntax_error\"]\n",
    "\n",
    "        # Calculate base metrics\n",
    "        f1, precision, recall, is_invalid = self._calculate_metrics(regex_str)\n",
    "        if is_invalid:\n",
    "            return self.penalty_weights[\"syntax_error\"]\n",
    "\n",
    "        # Calculate components\n",
    "        reward_components = {\n",
    "            \"f1\": f1 * self.penalty_weights[\"f1\"],\n",
    "            \"precision\": precision * self.penalty_weights[\"precision\"],\n",
    "            \"recall\": recall * self.penalty_weights[\"recall\"],\n",
    "            # 'complexity': len(regex_str) * self.penalty_weights['complexity'],\n",
    "            \"length_penalty\": len(regex_actions) * self.penalty_weights[\"length_penalty\"],\n",
    "        }\n",
    "\n",
    "        # Full match bonus\n",
    "        # if f1 >= 0.99:\n",
    "        #     reward_components[\"full_match\"] = self.penalty_weights[\"full_match\"]\n",
    "\n",
    "        # # Partial progress bonus (compare with previous attempts)\n",
    "        # if len(self.regex_history) > 1:\n",
    "        #     prev_f1 = self._calculate_metrics(self.regex_history[-2])[0] or 0\n",
    "        #     reward_components[\"partial_progress\"] = self.penalty_weights[\n",
    "        #         \"partial_progress\"\n",
    "        #     ] * (f1 - prev_f1)\n",
    "\n",
    "        # Total reward calculation\n",
    "        total_reward = sum(reward_components.values())\n",
    "\n",
    "        # Apply non-linear scaling\n",
    "        return np.sign(total_reward) * np.log1p(np.abs(total_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb60e4a5d73d4efe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T21:27:56.004150200Z",
     "start_time": "2025-04-16T21:18:47.247657Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")  # \"cuda\" if torch.cuda.is_available() else\n",
    "len(ACTIONS)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c1be9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f36d8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ACTIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5d34c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df3a616",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, observation_space: int, action_space: int, hidden_dim: int = 128):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.input_layer = nn.Linear(observation_space, hidden_dim)\n",
    "        self.layer1 = nn.Linear(hidden_dim, hidden_dim * 2)\n",
    "        self.layer2 = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.output_layer = nn.Linear(hidden_dim, action_space)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.input_layer(x))\n",
    "        x = F.leaky_relu(self.layer1(x))\n",
    "        x = F.leaky_relu(self.layer2(x))\n",
    "        actions = self.output_layer(x)\n",
    "        return F.softmax(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b557ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2CNet(nn.Module):\n",
    "    def __init__(self, input_dim: int, output_dim: int, hidden_dim: int = 128) -> None:\n",
    "        super(A2CNet, self).__init__()\n",
    "\n",
    "        self.body = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.policy = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim * 2, output_dim),\n",
    "        )\n",
    "\n",
    "        self.value = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        body_out = self.body(x)\n",
    "        return self.policy(body_out), self.value(body_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8308c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import random\n",
    "\n",
    "\n",
    "def select_action(policy_network: nn.Module, env: Environment, on_policy=bool):\n",
    "    state = env.get_state_tensor().to(device)\n",
    "    action_probs = policy_network(state).squeeze()\n",
    "    log_probs = torch.log(action_probs)\n",
    "    cpu_action_probs = action_probs.detach().cpu().numpy()\n",
    "    if on_policy:\n",
    "        action = np.argmax(cpu_action_probs)\n",
    "    else:\n",
    "        action = np.random.choice(np.arange(len(ACTIONS)), p=cpu_action_probs)\n",
    "\n",
    "    return action, log_probs, action_probs\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def choose_action(self, action_logits):\n",
    "        return random.choices(range(len(action_logits)), F.softmax(action_logits, dim=0))[\n",
    "            0\n",
    "        ]\n",
    "\n",
    "    def choose_optimal_action(self, action_logits) -> int:\n",
    "        return int(np.argmax(F.softmax(action_logits, dim=0).cpu()).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f2726c",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_network = PolicyNetwork(Environment.max_steps, len(ACTIONS)).to(device)\n",
    "\n",
    "gamma = 0.99\n",
    "lr_policy_net = 2**-13\n",
    "optimizer = torch.optim.Adam(policy_network.parameters(), lr=lr_policy_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430a24fdd97a0c",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "train_data = \"\"\"a45b \"\"\"\n",
    "array = [x.span() for x in re.finditer(\"[1-9]\", train_data)]\n",
    "\n",
    "target = [0 for _ in range(len(train_data))]\n",
    "for it in array:\n",
    "    for i in range(it[0], it[1]):\n",
    "        target[i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29f18d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4820fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum(np.bitwise_xor([0, 0, 1, 0], target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda3146170d268e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment(train_data, target, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ce0361",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score = -HIGHEST\n",
    "best_env = env.reset()\n",
    "NUM_EPISODES = 5000\n",
    "scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d406bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "\n",
    "EVAL_INTERVAL = 500\n",
    "cache_hits = 0\n",
    "\n",
    "loop = tqdm(range(NUM_EPISODES))\n",
    "for episode in loop:\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    scores.append([-HIGHEST])\n",
    "    prev_reward = 0\n",
    "    cumulative_discount = 1.0\n",
    "    prev_states = [env.get_state()]\n",
    "\n",
    "    on_policy = (episode + 1) % EVAL_INTERVAL == 0\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    episode_reward = 0\n",
    "    episode_log_probs = []\n",
    "    rewards = []\n",
    "\n",
    "    # run episode\n",
    "    while not done:\n",
    "        action, actions_log_probabilities, _ = select_action(\n",
    "            policy_network, env, on_policy\n",
    "        )\n",
    "        # print(\"Action\", action, IDX_TO_ACTIONS[action])\n",
    "        episode_log_probs.append(actions_log_probabilities[action])\n",
    "\n",
    "        next_state, new_score, done = env.step(action)\n",
    "\n",
    "        if new_score > best_score:\n",
    "            best_score = new_score\n",
    "            best_env = copy(next_state)\n",
    "            print(\n",
    "                \"Best Action\",\n",
    "                action,\n",
    "                IDX_TO_ACTIONS[action],\n",
    "                best_env,\n",
    "                best_score,\n",
    "                env.regexp,\n",
    "            )\n",
    "\n",
    "        prev_score = scores[episode][-1]\n",
    "\n",
    "        # reward = round((new_score - prev_score), 2)\n",
    "        reward = new_score\n",
    "        rewards.append(reward)\n",
    "        episode_reward += reward\n",
    "\n",
    "        scores[episode].append(new_score)\n",
    "\n",
    "        if done:\n",
    "            env.reset()\n",
    "\n",
    "    if not on_policy:\n",
    "        discounted_returns = []\n",
    "        running_return = 0\n",
    "\n",
    "        # Reverse rewards and calculate cumulative discounted returns\n",
    "        for r in reversed(rewards):\n",
    "            running_return = r + 0.99 * running_return\n",
    "            discounted_returns.insert(0, running_return)\n",
    "\n",
    "        returns_tensor = torch.tensor(discounted_returns)\n",
    "        # returns_tensor = (returns_tensor - returns_tensor.mean()) / (returns_tensor.std() + 1e-9)\n",
    "\n",
    "        policy_loss = []\n",
    "        for log_prob, Gt in zip(episode_log_probs, returns_tensor):\n",
    "            policy_loss.append(-log_prob * Gt)\n",
    "        total_loss = torch.stack(policy_loss).sum()\n",
    "\n",
    "        # num_steps = len(episode_log_probs)\n",
    "        # per_timestep_losses = [\n",
    "        #     -log_prob * episode_reward for log_prob in episode_log_probs\n",
    "        # ]\n",
    "        # total_loss = torch.stack(per_timestep_losses).sum()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    loop.set_postfix({\"best\": best_score})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d470a9",
   "metadata": {},
   "source": [
    "precision, recall, f1\n",
    "\n",
    "accuracy reward = 10 * (new_score-score)\n",
    "f1 reward = 5 * f1\n",
    "complexity penalty = -0.3 (len regexp)\n",
    "bonus = 50 if f1 >= 0.99 else 0\n",
    "syntax penalty = -15 if invalid syntax\n",
    "converges bonus = 2 * log(1+env.valid_matches - env.false_positives)\n",
    "Total = max(sum, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb72ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "IDX_TO_ACTIONS[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38831535",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2263b458",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e59262c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "\n",
    "\n",
    "def plot_run(scores: list[float], figsize: tuple[int, int] = (20, 9)):\n",
    "    fig, ax1 = plt.subplots(1, 1, figsize=figsize)\n",
    "    cmap = plt.get_cmap(\"jet\", len(scores))\n",
    "    for i in range(len(scores)):\n",
    "        ax1.plot(scores[i], c=cmap(i))\n",
    "    norm = mpl.colors.Normalize(vmin=0, vmax=len(scores))\n",
    "    sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "    sm.set_array([])\n",
    "    plt.colorbar(sm, ticks=np.arange(len(scores)), ax=ax1)\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def split_scores(scores):\n",
    "    odd = []\n",
    "    even = []\n",
    "    for i in range(len(scores)):\n",
    "        if (i + 1) % EVAL_INTERVAL == 0:\n",
    "            odd.append(scores[i])\n",
    "        else:\n",
    "            even.append(scores[i])\n",
    "    return odd, even\n",
    "\n",
    "\n",
    "def plot_scores(scores: list[float], figsize: tuple[int, int] = (16, 9)):\n",
    "    best_scores = [max(x) for x in scores]\n",
    "    worst_scores = [min(x) for x in scores]\n",
    "    mean_scores = [np.mean(x) for x in scores]\n",
    "    qwerty_scores = [x[0] for x in scores]\n",
    "\n",
    "    plt.subplots(1, 1, figsize=figsize)\n",
    "    plt.plot(best_scores, label=\"Best score\")\n",
    "    plt.plot(worst_scores, label=\"Worst score\")\n",
    "    plt.plot(mean_scores, label=\"Mean score\")\n",
    "    plt.plot(qwerty_scores, label=\"QWERTY score\")\n",
    "\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    # plt.xlim([0, 500])\n",
    "    # plt.ylim([best_score, 14000])\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.title(\"Scores for each episode\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_scores(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f741f8e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
